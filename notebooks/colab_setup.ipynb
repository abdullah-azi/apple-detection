{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üçé Apple Detection - Google Colab Setup (Complete Guide)\n",
        "\n",
        "This notebook provides a **complete setup from scratch** - no Kaggle account needed!\n",
        "\n",
        "## üìã Steps:\n",
        "1. **Enable GPU runtime** (Important!)\n",
        "2. **Upload project files** (or clone from GitHub)\n",
        "3. **Install dependencies**\n",
        "4. **Download Fruit Detection Dataset** (no account needed)\n",
        "5. **Filter dataset** to extract only apple images\n",
        "6. **Train the model**\n",
        "7. **Run inference**\n",
        "8. **Save to Google Drive**\n",
        "\n",
        "---\n",
        "\n",
        "## üéØ What This Notebook Does:\n",
        "- ‚úÖ Downloads the Fruit Detection Dataset (8479 images, 6 fruits)\n",
        "- ‚úÖ Filters to extract ONLY apple images automatically\n",
        "- ‚úÖ Splits into train/val/test sets\n",
        "- ‚úÖ Prepares everything for training\n",
        "- ‚úÖ No Kaggle account required!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Enable GPU Runtime\n",
        "\n",
        "**Important:** Before running any cells, enable GPU:\n",
        "1. Go to **Runtime** ‚Üí **Change runtime type**\n",
        "2. Set **Hardware accelerator** to **GPU** (T4)\n",
        "3. Click **Save**\n",
        "\n",
        "Let's verify GPU is available:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"‚úÖ GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"   CUDA Version: {torch.version.cuda}\")\n",
        "    print(f\"   GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  GPU not available. Please enable GPU runtime.\")\n",
        "    print(\"   Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Clone or Upload Project\n",
        "\n",
        "### Option A: Clone from GitHub (if you have a repository)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Clone from GitHub (if you have a repository)\n",
        "# Uncomment and modify:\n",
        "# !git clone https://github.com/yourusername/apple-detection.git\n",
        "# %cd apple-detection\n",
        "\n",
        "# Option 2: Upload project as ZIP (see next cell)\n",
        "print(\"üì¶ If you have a GitHub repo, uncomment the lines above.\")\n",
        "print(\"   Otherwise, upload your project as a ZIP file in the next cell.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Upload Project Files\n",
        "\n",
        "If you don't have a GitHub repo, you can upload your project files:\n",
        "1. Create a zip file of your project (excluding `venv/`, `__pycache__/`, etc.)\n",
        "2. Upload it using the cell below\n",
        "3. Extract it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"üì§ Upload your project ZIP file:\")\n",
        "print(\"   1. Create a ZIP of your project (exclude venv/, __pycache__/, .git/)\")\n",
        "print(\"   2. Click 'Choose Files' below\")\n",
        "print(\"   3. Select your ZIP file\")\n",
        "\n",
        "# Upload zip file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract if zip file was uploaded\n",
        "project_dir = None\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"\\nüì¶ Extracting {filename}...\")\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content')\n",
        "        print(f\"‚úÖ Extracted to /content/\")\n",
        "        \n",
        "        # Try to find project directory\n",
        "        extracted_dirs = [d for d in Path('/content').iterdir() if d.is_dir() and d.name != '__MACOSX']\n",
        "        if extracted_dirs:\n",
        "            project_dir = extracted_dirs[0]\n",
        "            print(f\"‚úÖ Project directory: {project_dir}\")\n",
        "        \n",
        "        # Remove zip file\n",
        "        os.remove(filename)\n",
        "\n",
        "# Set project directory\n",
        "if project_dir:\n",
        "    PROJECT_DIR = project_dir\n",
        "else:\n",
        "    PROJECT_DIR = Path('/content/apple-detection')\n",
        "    PROJECT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"\\nüìÅ Project directory: {PROJECT_DIR}\")\n",
        "print(\"‚úÖ Project uploaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Install Dependencies\n",
        "\n",
        "Install all required packages for the project:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install PyTorch with CUDA support (Colab usually has this, but we'll ensure it)\n",
        "print(\"üì¶ Installing PyTorch with CUDA...\")\n",
        "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118\n",
        "\n",
        "# Install other dependencies\n",
        "print(\"\\nüì¶ Installing project dependencies...\")\n",
        "import os\n",
        "req_file = PROJECT_DIR / 'requirements.txt'\n",
        "if req_file.exists():\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', '-r', str(req_file)], check=True)\n",
        "else:\n",
        "    # If requirements.txt doesn't exist, install manually\n",
        "    print(\"‚ö†Ô∏è  requirements.txt not found, installing manually...\")\n",
        "    import subprocess\n",
        "    subprocess.run(['pip', 'install', 'numpy', 'opencv-python', 'pillow', 'pyyaml', \n",
        "                    'tqdm', 'matplotlib', 'seaborn', 'albumentations'], check=True)\n",
        "\n",
        "# Verify installation\n",
        "print(\"\\n‚úÖ Verifying installation...\")\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import cv2\n",
        "import yaml\n",
        "\n",
        "print(\"\\n‚úÖ All dependencies installed successfully!\")\n",
        "print(f\"   PyTorch: {torch.__version__}\")\n",
        "print(f\"   Torchvision: {torchvision.__version__}\")\n",
        "print(f\"   NumPy: {np.__version__}\")\n",
        "print(f\"   OpenCV: {cv2.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Download Fruit Detection Dataset\n",
        "\n",
        "**No Kaggle account needed!** We'll download the dataset directly.\n",
        "\n",
        "### Option A: Download from Direct Link (Recommended)\n",
        "\n",
        "If you have a direct download link to the dataset ZIP file:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import zipfile\n",
        "from pathlib import Path\n",
        "\n",
        "# ============================================\n",
        "# METHOD 1: Direct Download Link\n",
        "# ============================================\n",
        "# If you have a direct download link, paste it here:\n",
        "DATASET_URL = \"\"  # Paste your dataset download URL here\n",
        "\n",
        "if DATASET_URL:\n",
        "    print(\"üì• Downloading dataset from URL...\")\n",
        "    import subprocess\n",
        "    subprocess.run(['wget', '-O', 'fruit-dataset.zip', DATASET_URL], check=True)\n",
        "    \n",
        "    print(\"üì¶ Extracting dataset...\")\n",
        "    with zipfile.ZipFile('fruit-dataset.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "    \n",
        "    os.remove('fruit-dataset.zip')\n",
        "    print(\"‚úÖ Dataset downloaded and extracted!\")\n",
        "    \n",
        "    # Find extracted directory\n",
        "    extracted_dirs = [d for d in Path('/content').iterdir() \n",
        "                      if d.is_dir() and 'fruit' in d.name.lower()]\n",
        "    if extracted_dirs:\n",
        "        DATASET_PATH = extracted_dirs[0]\n",
        "    else:\n",
        "        DATASET_PATH = Path('/content/fruit-dataset')\n",
        "    print(f\"‚úÖ Dataset path: {DATASET_PATH}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No URL provided. Use Option B or C below.\")\n",
        "    DATASET_PATH = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option B: Upload Dataset ZIP File\n",
        "\n",
        "Upload the dataset ZIP file from your computer:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "print(\"üì§ Upload the Fruit Detection Dataset ZIP file:\")\n",
        "print(\"   1. Download the dataset from the source\")\n",
        "print(\"   2. Click 'Choose Files' below\")\n",
        "print(\"   3. Select the ZIP file\")\n",
        "\n",
        "# Upload dataset zip file\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Extract dataset\n",
        "DATASET_PATH = None\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.zip'):\n",
        "        print(f\"\\nüì¶ Extracting {filename}...\")\n",
        "        extract_dir = Path('/content') / filename.replace('.zip', '')\n",
        "        with zipfile.ZipFile(filename, 'r') as zip_ref:\n",
        "            zip_ref.extractall('/content')\n",
        "        \n",
        "        # Find extracted directory\n",
        "        extracted_dirs = [d for d in Path('/content').iterdir() \n",
        "                          if d.is_dir() and d.name != '__MACOSX' \n",
        "                          and (d.name == filename.replace('.zip', '') or 'fruit' in d.name.lower())]\n",
        "        if extracted_dirs:\n",
        "            DATASET_PATH = extracted_dirs[0]\n",
        "        else:\n",
        "            DATASET_PATH = extract_dir\n",
        "        \n",
        "        print(f\"‚úÖ Dataset extracted to: {DATASET_PATH}\")\n",
        "        os.remove(filename)\n",
        "        break\n",
        "\n",
        "if DATASET_PATH:\n",
        "    print(f\"\\n‚úÖ Dataset ready at: {DATASET_PATH}\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No dataset found. Please upload the ZIP file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Option C: Mount Google Drive\n",
        "\n",
        "If your dataset is already in Google Drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "from pathlib import Path\n",
        "\n",
        "# Mount Google Drive\n",
        "print(\"üîó Mounting Google Drive...\")\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set path to dataset in Drive (adjust as needed)\n",
        "DRIVE_DATASET_PATH = \"/content/drive/MyDrive/fruit-detection-dataset\"  # Adjust this path\n",
        "\n",
        "if Path(DRIVE_DATASET_PATH).exists():\n",
        "    DATASET_PATH = Path(DRIVE_DATASET_PATH)\n",
        "    print(f\"‚úÖ Found dataset in Drive: {DATASET_PATH}\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Dataset not found at: {DRIVE_DATASET_PATH}\")\n",
        "    print(\"   Please adjust DRIVE_DATASET_PATH above or upload the dataset.\")\n",
        "    DATASET_PATH = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Filter Dataset for Apple Images Only\n",
        "\n",
        "Now we'll filter the multi-fruit dataset to extract ONLY apple images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First, let's check if we have the dataset\n",
        "if 'DATASET_PATH' not in locals() or DATASET_PATH is None:\n",
        "    print(\"‚ùå Error: Dataset not found!\")\n",
        "    print(\"   Please complete Step 4 first (download/upload dataset)\")\n",
        "else:\n",
        "    print(f\"üìÅ Dataset path: {DATASET_PATH}\")\n",
        "    \n",
        "    # Check dataset structure\n",
        "    print(\"\\nüîç Checking dataset structure...\")\n",
        "    if DATASET_PATH.exists():\n",
        "        items = list(DATASET_PATH.iterdir())[:10]\n",
        "        print(f\"   Found {len(list(DATASET_PATH.iterdir()))} items\")\n",
        "        for item in items:\n",
        "            if item.is_dir():\n",
        "                file_count = len(list(item.rglob('*.jpg')) + list(item.rglob('*.png')))\n",
        "                print(f\"   üìÅ {item.name}/ ({file_count} images)\")\n",
        "            else:\n",
        "                print(f\"   üìÑ {item.name}\")\n",
        "    else:\n",
        "        print(f\"   ‚ùå Path does not exist: {DATASET_PATH}\")\n",
        "    \n",
        "    # Now run the filtering script\n",
        "    print(\"\\nüçé Starting to filter for apple images...\")\n",
        "    print(\"   This will extract only images containing apples.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Copy the filtering script to Colab\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Create scripts directory if it doesn't exist\n",
        "scripts_dir = PROJECT_DIR / 'scripts'\n",
        "scripts_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Read the filtering script (we'll create it inline)\n",
        "filter_script = \"\"\"\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "\n",
        "def find_dataset_structure(dataset_path):\n",
        "    dataset_path = Path(dataset_path)\n",
        "    possible_structures = [\n",
        "        (dataset_path / 'images', dataset_path / 'labels'),\n",
        "        (dataset_path / 'train' / 'images', dataset_path / 'train' / 'labels'),\n",
        "        (dataset_path / 'images' / 'train', dataset_path / 'labels' / 'train'),\n",
        "        (dataset_path / 'train', dataset_path / 'train'),\n",
        "    ]\n",
        "    \n",
        "    for img_path, lbl_path in possible_structures:\n",
        "        if img_path.exists():\n",
        "            label_locations = [\n",
        "                lbl_path,\n",
        "                img_path.parent / 'labels',\n",
        "                dataset_path / 'labels',\n",
        "                img_path.parent.parent / 'labels',\n",
        "            ]\n",
        "            for label_path in label_locations:\n",
        "                if label_path.exists():\n",
        "                    return img_path, label_path\n",
        "    \n",
        "    if (dataset_path / 'images').exists():\n",
        "        return dataset_path / 'images', dataset_path / 'labels'\n",
        "    return None, None\n",
        "\n",
        "def analyze_classes(labels_dir, sample_size=100):\n",
        "    print(\"\\\\nüìä Analyzing class distribution...\")\n",
        "    class_counts = Counter()\n",
        "    annotation_files = list(labels_dir.glob('*.txt')) or list(labels_dir.rglob('*.txt'))\n",
        "    sample_files = annotation_files[:min(sample_size, len(annotation_files))]\n",
        "    \n",
        "    for ann_file in sample_files:\n",
        "        try:\n",
        "            with open(ann_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if parts:\n",
        "                        class_counts[parts[0]] += 1\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    fruit_names = {'0': 'Apple', '1': 'Grapes', '2': 'Pineapple', '3': 'Orange', '4': 'Banana', '5': 'Watermelon'}\n",
        "    for class_id in sorted(class_counts.keys()):\n",
        "        fruit_name = fruit_names.get(class_id, f'Class {class_id}')\n",
        "        print(f\"  Class {class_id} ({fruit_name}): {class_counts[class_id]} boxes\")\n",
        "    \n",
        "    return '0'\n",
        "\n",
        "def filter_apple_annotations(input_ann_path, output_ann_path, apple_class_id='0'):\n",
        "    apple_boxes = []\n",
        "    try:\n",
        "        with open(input_ann_path, 'r') as f:\n",
        "            for line in f:\n",
        "                parts = line.strip().split()\n",
        "                if parts and parts[0] == apple_class_id:\n",
        "                    apple_boxes.append(line.strip())\n",
        "        if apple_boxes:\n",
        "            with open(output_ann_path, 'w') as f:\n",
        "                for box in apple_boxes:\n",
        "                    f.write(box + '\\\\n')\n",
        "            return True\n",
        "    except:\n",
        "        pass\n",
        "    return False\n",
        "\n",
        "def find_corresponding_annotation(image_path, labels_dir):\n",
        "    possible_paths = [\n",
        "        labels_dir / (image_path.stem + '.txt'),\n",
        "        labels_dir / image_path.name.replace(image_path.suffix, '.txt'),\n",
        "        image_path.parent.parent / 'labels' / (image_path.stem + '.txt'),\n",
        "        image_path.parent / (image_path.stem + '.txt'),\n",
        "    ]\n",
        "    for path in possible_paths:\n",
        "        if path.exists():\n",
        "            return path\n",
        "    return None\n",
        "\n",
        "# Main filtering function\n",
        "def filter_and_prepare_dataset(dataset_path, output_dir, apple_class_id='0', seed=42):\n",
        "    dataset_path = Path(dataset_path)\n",
        "    output_dir = Path(output_dir)\n",
        "    \n",
        "    for split in ['train', 'val', 'test']:\n",
        "        (output_dir / 'images' / split).mkdir(parents=True, exist_ok=True)\n",
        "        (output_dir / 'annotations' / split).mkdir(parents=True, exist_ok=True)\n",
        "    \n",
        "    images_dir, labels_dir = find_dataset_structure(dataset_path)\n",
        "    if not images_dir or not images_dir.exists():\n",
        "        print(f\"‚ùå Error: Could not find images directory\")\n",
        "        return False\n",
        "    \n",
        "    if not labels_dir or not labels_dir.exists():\n",
        "        label_files = list(dataset_path.rglob('*.txt'))\n",
        "        if label_files:\n",
        "            labels_dir = label_files[0].parent\n",
        "        else:\n",
        "            print(\"‚ùå Error: No annotation files found!\")\n",
        "            return False\n",
        "    \n",
        "    print(f\"‚úÖ Found images in: {images_dir}\")\n",
        "    print(f\"‚úÖ Found labels in: {labels_dir}\")\n",
        "    \n",
        "    apple_class_id = analyze_classes(labels_dir)\n",
        "    \n",
        "    all_images = list(set(\n",
        "        list(images_dir.glob('*.jpg')) + list(images_dir.glob('*.png')) +\n",
        "        list(images_dir.glob('*.JPG')) + list(images_dir.glob('*.PNG')) +\n",
        "        list(images_dir.rglob('*.jpg')) + list(images_dir.rglob('*.png'))\n",
        "    ))\n",
        "    print(f\"\\\\nFound {len(all_images)} total images\")\n",
        "    \n",
        "    apple_images = []\n",
        "    for img_path in all_images:\n",
        "        ann_path = find_corresponding_annotation(img_path, labels_dir)\n",
        "        if ann_path and ann_path.exists():\n",
        "            temp_ann = output_dir / 'temp_check.txt'\n",
        "            if filter_apple_annotations(ann_path, temp_ann, apple_class_id):\n",
        "                apple_images.append((img_path, ann_path))\n",
        "            if temp_ann.exists():\n",
        "                temp_ann.unlink()\n",
        "    \n",
        "    print(f\"‚úÖ Found {len(apple_images)} images containing apples\")\n",
        "    \n",
        "    if len(apple_images) == 0:\n",
        "        print(\"‚ùå Error: No apple images found!\")\n",
        "        return False\n",
        "    \n",
        "    random.seed(seed)\n",
        "    random.shuffle(apple_images)\n",
        "    \n",
        "    train_count = int(0.7 * len(apple_images))\n",
        "    val_count = int(0.15 * len(apple_images))\n",
        "    \n",
        "    train_data = apple_images[:train_count]\n",
        "    val_data = apple_images[train_count:train_count + val_count]\n",
        "    test_data = apple_images[train_count + val_count:]\n",
        "    \n",
        "    print(f\"\\\\nüì¶ Split: Train={len(train_data)}, Val={len(val_data)}, Test={len(test_data)}\")\n",
        "    \n",
        "    def copy_split(data_list, split_name):\n",
        "        for img_path, ann_path in data_list:\n",
        "            dest_img = output_dir / 'images' / split_name / img_path.name\n",
        "            shutil.copy(img_path, dest_img)\n",
        "            dest_ann = output_dir / 'annotations' / split_name / (img_path.stem + '.txt')\n",
        "            filter_apple_annotations(ann_path, dest_ann, apple_class_id)\n",
        "        print(f\"  ‚úÖ {split_name}: {len(data_list)} images\")\n",
        "    \n",
        "    copy_split(train_data, 'train')\n",
        "    copy_split(val_data, 'val')\n",
        "    copy_split(test_data, 'test')\n",
        "    \n",
        "    print(\"\\\\nüéâ Dataset ready!\")\n",
        "    return True\n",
        "\n",
        "# Run the filtering\n",
        "if 'DATASET_PATH' in locals() and DATASET_PATH:\n",
        "    OUTPUT_DIR = PROJECT_DIR / 'data'\n",
        "    success = filter_and_prepare_dataset(DATASET_PATH, OUTPUT_DIR)\n",
        "    if success:\n",
        "        print(f\"\\\\n‚úÖ Filtered dataset saved to: {OUTPUT_DIR}\")\n",
        "    else:\n",
        "        print(\"\\\\n‚ùå Filtering failed. Please check the dataset structure.\")\n",
        "else:\n",
        "    print(\"‚ùå Please complete Step 4 first to download/upload the dataset.\")\n",
        "\"\"\"\n",
        "\n",
        "# Save and execute the script\n",
        "script_path = scripts_dir / 'filter_dataset.py'\n",
        "with open(script_path, 'w') as f:\n",
        "    f.write(filter_script)\n",
        "\n",
        "print(\"‚úÖ Filtering script created\")\n",
        "exec(filter_script)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Verify Filtered Dataset\n",
        "\n",
        "Let's check the final dataset structure:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "\n",
        "data_dir = PROJECT_DIR / 'data'\n",
        "\n",
        "print(\"üìÅ Final Dataset Structure:\")\n",
        "print(f\"   Root: {data_dir}\")\n",
        "\n",
        "total_images = 0\n",
        "total_boxes = 0\n",
        "\n",
        "for split in ['train', 'val', 'test']:\n",
        "    img_dir = data_dir / 'images' / split\n",
        "    ann_dir = data_dir / 'annotations' / split\n",
        "    \n",
        "    img_count = len(list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))) if img_dir.exists() else 0\n",
        "    ann_count = len(list(ann_dir.glob('*.txt'))) if ann_dir.exists() else 0\n",
        "    \n",
        "    # Count total apple boxes\n",
        "    boxes = 0\n",
        "    if ann_dir.exists():\n",
        "        for ann_file in ann_dir.glob('*.txt'):\n",
        "            with open(ann_file, 'r') as f:\n",
        "                boxes += len([l for l in f if l.strip()])\n",
        "    \n",
        "    total_images += img_count\n",
        "    total_boxes += boxes\n",
        "    \n",
        "    print(f\"\\n{split.upper()}:\")\n",
        "    print(f\"  Images: {img_count}\")\n",
        "    print(f\"  Annotations: {ann_count}\")\n",
        "    print(f\"  Apple bounding boxes: {boxes}\")\n",
        "    if img_count > 0:\n",
        "        print(f\"  Avg boxes per image: {boxes/img_count:.2f}\")\n",
        "\n",
        "print(f\"\\nüìä Summary:\")\n",
        "print(f\"  Total images: {total_images}\")\n",
        "print(f\"  Total apple boxes: {total_boxes}\")\n",
        "if total_images > 0:\n",
        "    print(f\"  Average boxes per image: {total_boxes/total_images:.2f}\")\n",
        "\n",
        "if total_images > 0:\n",
        "    print(\"\\n‚úÖ Dataset is ready for training!\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No images found. Please check the filtering process.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Train the Model\n",
        "\n",
        "Now you're ready to train! Use the Colab-optimized configuration:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add src to path\n",
        "import sys\n",
        "sys.path.insert(0, str(PROJECT_DIR / 'src'))\n",
        "\n",
        "# Check if config file exists\n",
        "config_path = PROJECT_DIR / 'configs' / 'config_colab.yaml'\n",
        "if not config_path.exists():\n",
        "    print(\"‚ö†Ô∏è  config_colab.yaml not found. Creating it...\")\n",
        "    config_path.parent.mkdir(exist_ok=True)\n",
        "    # You may need to create the config file manually or use the existing one\n",
        "\n",
        "print(f\"üìÅ Project directory: {PROJECT_DIR}\")\n",
        "print(f\"üìÅ Config file: {config_path}\")\n",
        "\n",
        "# Train the model\n",
        "print(\"\\nüöÄ Ready to train!\")\n",
        "print(\"   Uncomment the line below to start training:\")\n",
        "print(f\"   !python {PROJECT_DIR}/src/train.py --config {config_path}\")\n",
        "\n",
        "# Uncomment to start training:\n",
        "# !python {PROJECT_DIR}/src/train.py --config {config_path}\n",
        "\n",
        "# Or if you prefer to run in Python:\n",
        "# from src.train import train_model\n",
        "# train_model(config_path=str(config_path))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Run Inference\n",
        "\n",
        "Test your trained model on new images:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Upload a test image\n",
        "from google.colab import files\n",
        "from IPython.display import Image, display\n",
        "\n",
        "print(\"üì§ Upload a test image to run inference:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Run inference (adjust paths as needed)\n",
        "# Uncomment to run inference:\n",
        "# !python {PROJECT_DIR}/src/inference.py \\\n",
        "#     --image /content/your_test_image.jpg \\\n",
        "#     --checkpoint {PROJECT_DIR}/checkpoints/best_model.pth \\\n",
        "#     --output {PROJECT_DIR}/results/detection.jpg \\\n",
        "#     --config {PROJECT_DIR}/configs/config_colab.yaml\n",
        "\n",
        "# Display result\n",
        "# display(Image(f'{PROJECT_DIR}/results/detection.jpg'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save Your Work\n",
        "\n",
        "**Important:** Colab sessions are temporary. Save your checkpoints and results to Google Drive:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mount drive if not already mounted\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "try:\n",
        "    drive.mount('/content/drive')\n",
        "    print(\"‚úÖ Google Drive mounted\")\n",
        "except:\n",
        "    print(\"‚ö†Ô∏è  Drive already mounted or mount failed\")\n",
        "\n",
        "# Create backup directory in Drive\n",
        "backup_dir = Path('/content/drive/MyDrive/apple-detection-backup')\n",
        "backup_dir.mkdir(exist_ok=True)\n",
        "\n",
        "print(f\"\\nüíæ Saving to: {backup_dir}\")\n",
        "\n",
        "# Copy checkpoints\n",
        "checkpoints_dir = PROJECT_DIR / 'checkpoints'\n",
        "if checkpoints_dir.exists():\n",
        "    dest_checkpoints = backup_dir / 'checkpoints'\n",
        "    if dest_checkpoints.exists():\n",
        "        shutil.rmtree(dest_checkpoints)\n",
        "    shutil.copytree(checkpoints_dir, dest_checkpoints, dirs_exist_ok=True)\n",
        "    print(\"‚úÖ Checkpoints saved to Drive!\")\n",
        "\n",
        "# Copy results\n",
        "results_dir = PROJECT_DIR / 'results'\n",
        "if results_dir.exists():\n",
        "    dest_results = backup_dir / 'results'\n",
        "    if dest_results.exists():\n",
        "        shutil.rmtree(dest_results)\n",
        "    shutil.copytree(results_dir, dest_results, dirs_exist_ok=True)\n",
        "    print(\"‚úÖ Results saved to Drive!\")\n",
        "\n",
        "# Copy config for reference\n",
        "config_file = PROJECT_DIR / 'configs' / 'config_colab.yaml'\n",
        "if config_file.exists():\n",
        "    shutil.copy(config_file, backup_dir / 'config_colab.yaml')\n",
        "    print(\"‚úÖ Config saved to Drive!\")\n",
        "\n",
        "print(f\"\\n‚úÖ All files saved to: {backup_dir}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üí° Tips for Colab\n",
        "\n",
        "1. **Session Timeout**: Colab sessions disconnect after ~90 minutes of inactivity. Keep the tab active during training.\n",
        "\n",
        "2. **GPU Limits**: Free Colab has usage limits. If you hit them, wait a few hours or consider Colab Pro.\n",
        "\n",
        "3. **Save Frequently**: Always save checkpoints to Google Drive to avoid losing progress.\n",
        "\n",
        "4. **Large Datasets**: For large datasets, consider using Google Drive instead of uploading directly.\n",
        "\n",
        "5. **Monitor Training**: Use TensorBoard or print statements to monitor training progress.\n",
        "\n",
        "## üéâ You're All Set!\n",
        "\n",
        "Happy training! üçéüîç\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
